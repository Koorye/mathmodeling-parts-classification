### 2.1  用户搜索零部件时所用关键词特点的分析

用户的关键词可以大体分为两段，前段包含品牌和车型信息，后段包含器件、部件和零件信息。

### 6.1  模型的评价

**缺点：**

- 模型只是简单的把关键词切割成小词条，并没有分析词条的内容是什么信息。
- 模型简单的把用户所有搜索关键词存储起来，没有排除掉一些无用的助词、属性词
- 模型无法判断品牌等专有名词

### 6.2  模型的改进

##### 设置停用词词典

如果词条在停用词列表中就排除

```python
stop_words = ['的', '和', '中', '第', ...]  # 停用词列表
```



##### 通过词性推断词条是否有用

```python
    temp_list = jieba.posseg.cut(string)
    patterns = []
    for each in temp_list:  # 判断词性，排除无意义的词性
        if each.flag not in ['a', 'd', 'p', 'u']:  # 排除形容词、副词、助词、介词等词性
            patterns.append(each.word)
```

```shell
模拟用户第1次输入
输入关键词：PC56挖掘机的发动机和座椅
词汇： {'PC56': 1, '挖掘机': 1, '发动机': 1, '座椅': 1}
模拟用户第2次输入
输入关键词：0

Process finished with exit code 0
```

“的”“和”等词被排除，减少模型的冗余，提高精确度。

##### 从用户的搜索记录中推断出转有名词

思想：如果用户多次输入的语句中包含一样的短语，模型就推测这个短语可能是一个专有名词

```python
def get_brand_word(string1, string2, pattern_dic):
    brand_words = []
    pattern1 = jieba.lcut_for_search(string1)
    pattern2 = jieba.lcut_for_search(string2)
    for pattern in pattern1:
        if pattern not in pattern2:
            pattern1.remove(pattern)

    for i in range(2, len(pattern1)):
        for j in range(0, len(pattern1) - i):
            brand = ''
            for k in range(0, i):
                brand += pattern1[j + k]
            if brand not in pattern_dic and brand in string2:
                flag = True
                for each in brand_words:
                    if each in brand:
                        brand_words.remove(each)
                    if brand in each:
                        flag = False
                if flag:
                    brand_words.append(brand)
    return brand_words
```



```shell
模拟用户第1次输入
输入关键词：三菱重工挖掘机
专有名词： []
普通词汇： {'三菱': 1, '重工': 1, '挖掘机': 1}
模拟用户第2次输入
输入关键词：三菱重工卡车
专有名词： ['三菱重工']
普通词汇： {'挖掘机': 1, '卡车': 1}
模拟用户第3次输入
输入关键词：0

Process finished with exit code 0
```

第一次搜索时，用户输入**三菱重工**，**三菱**和**重工**分别作为两个词被记录

第二次搜索时，用户再次输入**三菱重工**，模型推断该词是一个专有名词，因此加入专有名词词库，并从普通词库中删去相关词。



优化部分算法

```python
import jieba.posseg


def get_brand_word(string1, string2, pattern_dic):
    brand_words = []
    pattern1 = jieba.lcut_for_search(string1)
    pattern2 = jieba.lcut_for_search(string2)
    for pattern in pattern1:
        if pattern not in pattern2:
            pattern1.remove(pattern)

    for i in range(2, len(pattern1)):
        for j in range(0, len(pattern1) - i):
            brand = ''
            for k in range(0, i):
                brand += pattern1[j + k]
            if brand not in pattern_dic and brand in string2:
                flag = True
                for each in brand_words:
                    if each in brand:
                        brand_words.remove(each)
                    if brand in each:
                        flag = False
                if flag:
                    brand_words.append(brand)

    return brand_words


stop_words = ['的', '和', '中', '第']  # 停用词列表
string_list = []  # 用户所有完整搜索记录存入列表
brand_list = []  # 专有名词列表
pattern_dic = {}
count = 1
while True:
    print("模拟用户第%d次输入" % count)
    string = input("输入关键词：")
    if string == '0':  # 退出条件
        break
    temp_list = jieba.posseg.cut(string)
    patterns = []
    for each in temp_list:  # 判断词性，排除无意义的词性
        if each.flag not in ['a', 'd', 'p', 'u']:  # 排除形容词、副词、助词、介词等词性
            patterns.append(each.word)

    for pattern in patterns:
        if pattern not in stop_words:  # 排除停用词
            if pattern not in pattern_dic:
                pattern_dic[pattern] = 1
            else:
                pattern_dic[pattern] += 1

    for each in string_list:
        temp_list = get_brand_word(each, string, pattern_dic)
        for each in temp_list:
            brand_list.append(each)
    string_list.append(string)

    temp_list = []
    for pattern in pattern_dic:
        for brand in brand_list:
            if pattern in brand:
                temp_list.append(pattern)
                break

    print(temp_list)
    for each in temp_list:
        pattern_dic.pop(each)
    print('专有名词：', brand_list)
    print('普通词汇：', pattern_dic)
    count += 1
```

